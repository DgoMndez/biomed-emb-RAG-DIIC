{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG: Retrieval-Augmented Generation\n",
    "\n",
    "## ¿Qué es RAG?\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) es una extensión de un LLM que combina la recuperación (Retrieval) con la generación (Generation) para mejorar la adecuación de las respuestas. Se trata de incorporar un módulo de recuperación al LLM generativo que se encarga de dada una query buscar documentos relevantes sobre ella en una fuente determinada (BD, corpus, online) y utlizarlos para crear una query más precisa que es la que se transmite al modelo generativo para generar la respuesta.\n",
    "\n",
    "Sus posibles impactos son:\n",
    "* Coste bajo respecto al entrenamiento.\n",
    "* Mejora de la precisión en las respuestas, al usar fuentes externas relacionadas con la query.\n",
    "* Fácil actualización: se puede actualizar solo la BD de documentos en lugar de reentrenar el modelo.\n",
    "\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "Nuestro objetivo es la especialización de LLMs en el dominio biomédico.\n",
    "\n",
    "El año pasado estuvimos trabajando en la especialización de los embeddings de BERT ([pritamdeka](https://huggingface.co/pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb)) en el dominio de [HPO](https://hpo.jax.org/). Para lo que construimos un corpus de abstracts de [PubMed](https://pubmed.ncbi.nlm.nih.gov/) etiquetados con su fenotipo de búsqueda, entrenamos el modelo mediante fine-tuning y evaluamos el aprendizaje (de la codificación de fenotipos) a lo largo del proceso. Los principales problemas / aspectos de mejora fueron:\n",
    "\n",
    "* El método de evaluación estaba centrado únicamente en la codificación de los fenotipos en embeddings. Se basaba en la comparación de la similitud coseno entre los embeddings de los términos (similitud inferida) con la similitud Lin de los fenotipos en HPO (similitud de referencia). Se requiere un método de evaluación más estandarizado que mida si el modelo comprende los textos biomédicos. Por ejemplo [MedSentEval](https://www.sciencedirect.com/science/article/pii/S1532046420300253?via%3Dihub).\n",
    "* El error de test en realidad era de validación, porque se usó el conjunto de test para encontrar los mejores hiperparámetros.\n",
    "* No se monitorizó la pérdida a lo largo del proceso. Aunque guarda cierta relación con el método de evaluación es diferente y puede aportar más información sobre si hay underfitting/overfitting.\n",
    "* El vocabulario del modelo no se expande mediante el fine-tuning, y muchos términos de HPO no están en el vocabulario de BERT.\n",
    "\n",
    "El nuevo enfoque para tratar la especialización de LLMs en medicina será el RAG. El módulo de recuperación debe buscar información relevante sobre la query en un corpus de documentos de PubMed. Si no se usa la Generación, se trata de un modelo de Recuperación Aumentada (Retrieval-Augmented). Este tipo de modelos son utilizados para clasificación, extracción de información y QA.\n",
    "\n",
    "## Librerías a utilizar\n",
    "\n",
    "Se propone usar alguna de las siguientes librerías de python para RAG:\n",
    "\n",
    "1. Langchain.\n",
    "2. Haystack.\n",
    "\n",
    "Las dos librerías permiten construir sistemas RAG integrando distintos modelos LLM, generativos o embedder de OpenAI, Cohere, Google, HuggingFace...\n",
    "\n",
    "### 1. Langchain\n",
    "\n",
    "\"LangChain es una biblioteca diseñada para ayudar a construir aplicaciones que integran modelos de lenguaje grandes (LLMs), como GPT, con fuentes de datos externas y flujos de trabajo personalizados. Ofrece una infraestructura flexible para crear aplicaciones que utilizan LLMs de manera eficiente, permitiendo conectar estos modelos con bases de datos, APIs y otros recursos externos, así como organizar la interacción en diferentes etapas o flujos.\" - ChatGPT\n",
    "\n",
    "[Langchain](https://www.langchain.com/) es una empresa de San Francisco dedicada a dar el soporte necesario a los negocios para construir aplicaciones sobre LLMs. Sus tres productos principales son:\n",
    "\n",
    "* [Langchain](https://www.langchain.com/langchain): es la librería para el desarrollo de las aplicaciones basadas en LLMs. Permite construir la estructura de la aplicación conectando los módulos de modelos, BBDD, fuentes externas, prompts... \n",
    "* [Langsmith](https://www.langchain.com/langsmith): asistencia al desarrollo de software basado en LLMs a lo largo de todo su ciclo de vida. Sirve para debug, colaboración, test y monitorización. Puede ser útil para la explicación y la evaluación de nuestro modelo.\n",
    "* [Langgraph](https://www.langchain.com/langgraph): API para el desarrollo de agentes autónomos. Sirven para la toma de decisiones o la asistencia. En principio no lo vamos a usar para nada.\n",
    "\n",
    "#### Términos y condiciones de uso.\n",
    "\n",
    "Según las FAQ, LangChain es open-source.\n",
    "\n",
    "Los otros servicios de esta empresa (LangGraph Cloud, LangSmith) están sujetos a una licencia que es poco restrictiva. El único punto delicado es:\n",
    "\n",
    "\"Customer may not directly or indirectly and may not authorize any third party to: (a) decompile, disassemble, reverse engineer, or otherwise attempt to derive the source code, structure, ideas, algorithms, or associated know-how of, the Licensed Platform, Documentation, or reconstruct, or discover, any hidden or non-public elements of the Licensed Platform (except to the extent expressly permitted by applicable law not withstanding this restriction);\"\n",
    "\n",
    "\"(d) use the Licensed Platform to develop a similar or competing product or service\"\n",
    "\n",
    "Pero no nos obstaculiza en nada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get started\n",
    "* Cargamos las variables de entorno: API keys y logging info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar .env\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "\n",
    "WS_ROUTE = \"../\"\n",
    "load_dotenv(WS_ROUTE + \"env/diic.env\")\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")\n",
    "\n",
    "if \"HF_API_KEY\" not in os.environ:\n",
    "    os.environ[\"HF_API_KEY\"] = getpass(\"Enter Hugging Face API key:\")\n",
    "    \n",
    "if \"COHERE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"COHERE_API_KEY\"] = getpass(\"Enter Cohere API key:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# General\\n%pip install --quiet --upgrade langchain langchain-community langchain-chroma\\n    \\n# OpenAI\\n%pip install -qU langchain-openai\\n    \\n# Cohere\\n%pip install -qU langchain-cohere\\n\\n# HuggingFace\\n%pip install -qU langchain-huggingface\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instalación\n",
    "\"\"\"\n",
    "# General\n",
    "%pip install --quiet --upgrade langchain langchain-community langchain-chroma\n",
    "    \n",
    "# OpenAI\n",
    "%pip install -qU langchain-openai\n",
    "    \n",
    "# Cohere\n",
    "%pip install -qU langchain-cohere\n",
    "\n",
    "# HuggingFace\n",
    "%pip install -qU langchain-huggingface\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Modelos de OpenAI.\n",
    "No tengo acceso a la API de OpenAI: salta el error \"Rate Limit Exceeded, insufficient quota\" para distintos modelos.\n",
    "Otra desventaja: según sus Términos y Condiciones de uso está prohibido usar el output de sus modelos para desarrollar otros modelos que compitan con OpenAI (\"Using Output to develop models that compete with OpenAI.\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo\n",
      "Rate limit exceeded: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "o1\n",
      "Rate limit exceeded: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    }
   ],
   "source": [
    "# Modelos de OpenAI\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llms = [ChatOpenAI(model_name=\"gpt-3.5-turbo\"), ChatOpenAI(model_name=\"o1\")]\n",
    "\n",
    "for llm in llms:\n",
    "    print(llm.model_name)\n",
    "    try:\n",
    "        llm = ChatOpenAI()\n",
    "        llm.invoke(\"Hello, world!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Rate limit exceeded: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n",
      "\n",
      "I really need to talk about this, but we already discussed so far.\n",
      "\n",
      "So many things about you that I don't really feel like writing about here. You already do your first episode with us but there are\n"
     ]
    }
   ],
   "source": [
    "# Alternativas: modelos accesibles\n",
    "\n",
    "# 1. Sentence transformers\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Cargar el modelo de Hugging Face\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Generar texto\n",
    "response = generator(\"Hello, world!\", max_length=50)\n",
    "print(response[0]['generated_text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u: Hello, world!\n",
      "Hello, how can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# 2. Cohere\n",
    "import cohere\n",
    "\n",
    "co = cohere.Client(os.environ[\"COHERE_API_KEY\"])\n",
    "\n",
    "# Generar texto\n",
    "response = co.generate(\n",
    "    model='command-r-plus-08-2024',\n",
    "    prompt='Hello, world!',\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "print(\"u: Hello, world!\")\n",
    "print(response.generations[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tutorial RAG Langchain\n",
    "Fuente: https://python.langchain.com/docs/tutorials/rag/\n",
    "https://github.com/langchain-ai/langchain/blob/master/docs/docs/tutorials/rag.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos Cohere para generar texto\n",
    "\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "llm = ChatCohere(model=\"command-r-plus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u: What is Task Decomposition?\n",
      "C: Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "\n",
      "Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n",
      "\n",
      "\n",
      "Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\n",
      "\n",
      "Fig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
      "The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\n",
      "Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\n",
      "-----\n",
      "Prompt: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "\n",
      "            Question: What is Task Decomposition?\n",
      "\n",
      "            Context: Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "\n",
      "Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n",
      "\n",
      "\n",
      "Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\n",
      "\n",
      "Fig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
      "The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\n",
      "Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\n",
      "Answer:\n",
      "-----\n",
      "R: Task decomposition is a technique used to break down complex tasks into smaller, more manageable steps, often employed to enhance model performance on challenging assignments. This method can be executed by LLMs through simple prompting, task-specific instructions, or human input.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings # No accesible\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "# 1. Indexing Load\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2. Indexing split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3. Indexing store\n",
    "# No tengo acceso a OpenAIEmbeddings\n",
    "#vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Usar Hugging Face Embeddings en lugar de OpenAIEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)\n",
    "\n",
    "# 4. Retrieve and 5. Generate\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "ptemplate = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\n",
    "            Question: {question}\\n\n",
    "            Context: {context}\\nAnswer:\"\"\"\n",
    "            \n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=ptemplate)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "query = \"What is Task Decomposition?\"\n",
    "print(f\"u: {query}\")\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "context = format_docs(retrieved_docs)\n",
    "print(\"C: \" + context)\n",
    "print(\"-----\")\n",
    "print(\"Prompt: \" + ptemplate.format(context=context, question=query))\n",
    "print(\"-----\")\n",
    "print(\"R: \" + response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Características de LangChain\n",
    "\n",
    "1. Fuentes externas: en el ejemplo anterior se accede a una web para conseguir los documentos que conforman la BD del retriever.\n",
    "2. Cadenas/chains: en el ejemplo anterior se crea una cadena de RAG formada por: retriever, formateador, prompt, llm y parser. Langchain permite crear flujos de trabajo tan complejos como se quiera usando los diferentes tipos de módulos. Hay un gran catálogo de retrievers, modelos y prompts a utilizar. Nos interesan los modelos de HuggingFace porque son de libre acceso. En el modelo por ejemplo se usó un embedder de HuggingFace, un Vector Retriever y un LLM generativo de Cohere. El LLM de OpenAI requiere una clave de acceso con suficiente cuota en el plan.\n",
    "3. Agentes: esta es la parte correspondiente a LangGraph que en principio no se va a usar. Sirve para que las cadenas tomen decisiones para dirigir el flujo de trabajo, por ejemplo cuánta información buscar, dónde buscarla, cuándo devolver una respuesta con la salida generada o cuándo iterar para mejorarla...\n",
    "4. Memoria: para agentes conversacionales, LangChain ofrece soporte para recordar las interacciones y adaptar así sus respuestas.\n",
    "\n",
    "Más información:\n",
    "* Deconstructing RAG: https://blog.langchain.dev/deconstructing-rag/.\n",
    "* RAG strategies: https://blog.langchain.dev/applying-openai-rag/.\n",
    "* RAG evaluation cookbook: https://github.com/langchain-ai/langchain/blob/master/cookbook/advanced_rag_eval.ipynb?ref=blog.langchain.dev\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Haystack\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haystack es una librería de código abierto de Deepset dedicada al mismo problema que Langchain: soportar el desarrollo de apps basadas en LLMs. Permite prácticamente la misma funcionalidad: crear flujos de trabajo combinando módulos como LLMs, embedders, retrievers y BD de documentos, donde para cada uno se tienen distintas opciones como HuggingFace, OpenAI o Cohere. También ofrece data ingestion (obtener los datos online), evaluación y monitorización.\n",
    "\n",
    "\n",
    "#### Tutorial RAG con Haystack\n",
    "[27_First_RAG_Pipeline.ipynb](./haystack/27_First_RAG_Pipeline.ipynb) basado en https://haystack.deepset.ai/tutorials/27_first_rag_pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a237f5f7e04e0c881a982328b5cf99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Database\n",
    "\n",
    "# Document Store\n",
    "\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from haystack import Document\n",
    "\n",
    "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
    "docs = [Document(content=doc[\"content\"], meta=doc[\"meta\"]) for doc in dataset]\n",
    "\n",
    "# Document embedder\n",
    "\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "\n",
    "doc_embedder = SentenceTransformersDocumentEmbedder(model=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
    "doc_embedder.warm_up()\n",
    "\n",
    "# Write documents\n",
    "\n",
    "docs_with_embeddings = doc_embedder.run(docs)\n",
    "document_store.write_documents(docs_with_embeddings[\"documents\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG components\n",
    "\n",
    "# 1. Embedder (same as Document Store)\n",
    "\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "\n",
    "text_embedder = SentenceTransformersTextEmbedder(model=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
    "\n",
    "# 2. Retriever\n",
    "\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "\n",
    "retriever = InMemoryEmbeddingRetriever(document_store)\n",
    "\n",
    "# 3. Prompt\n",
    "\n",
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "template = \"\"\"\n",
    "Given the following information, answer the question.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_builder = PromptBuilder(template=template)\n",
    "\n",
    "\n",
    "# 4. LLM Generator\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# No tengo acceso a OpenAI\n",
    "#from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.generators import HuggingFaceAPIGenerator\n",
    "from haystack.utils import Secret\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")\n",
    "\n",
    "if \"HF_API_KEY\" not in os.environ:\n",
    "    os.environ[\"HF_API_KEY\"] = getpass(\"Enter Hugging Face API key:\")\n",
    "\n",
    "#generator = OpenAIGenerator(model=\"text-embedding-ada-002\")\n",
    "generator = HuggingFaceAPIGenerator(api_type=\"serverless_inference_api\",\n",
    "                                    api_params={\"model\": \"HuggingFaceH4/zephyr-7b-beta\"},\n",
    "                                    token=Secret.from_token(os.environ[\"HF_API_KEY\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x729c61541050>\n",
       "🚅 Components\n",
       "  - text_embedder: SentenceTransformersTextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: HuggingFaceAPIGenerator\n",
       "🛤️ Connections\n",
       "  - text_embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG Pipeline\n",
    "\n",
    "from haystack import Pipeline\n",
    "\n",
    "basic_rag_pipeline = Pipeline()\n",
    "# Add components to your pipeline\n",
    "basic_rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "basic_rag_pipeline.add_component(\"retriever\", retriever)\n",
    "basic_rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "basic_rag_pipeline.add_component(\"llm\", generator)\n",
    "\n",
    "# Now, connect the components to each other\n",
    "basic_rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "basic_rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\") #.documents -> puntos de entrada\n",
    "basic_rag_pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868695995b7b4f78b5ef094ecfe16fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Rhodes Statue, also known as the Colossus of Rhodes, was a statue of the Greek sun-god Helios, erected in the city of Rhodes by Chares of Lindos in 280 BC. It was constructed to celebrate the successful defence of Rhodes city against an attack by Demetrius Poliorcetes, who had besieged it for a year with a large army and navy. According to most contemporary descriptions, the Colossus stood approximately 70 cubits, or 33 metres (108 feet) high – approximately the height of the modern Statue of Liberty from feet to crown – making it the tallest statue in the ancient world. It collapsed during the earthquake of 226 BC, although parts of it were preserved. In accordance with a certain oracle, the Rhodians did not build it again. John Malalas wrote that Hadrian in his reign re-erected the Colossus, but he was mistaken. According to the Suda, the Rhodians were called Colossaeans (Κολοσσαεῖς), because they erected the statue on the island. Today, the massive castle of the Knights Hospitaller (Knights of St. John) still stands in Bodrum, and the polished stone and marble blocks of the Mausoleum can be spotted built into the walls of the structure. At the site of the Mausoleum, only the foundation remains, and a small museum. Some of the surviving sculptures at the British Museum include fragments of statues and many slabs of the frieze showing the battle between the Greeks and the Amazons. There the images of Mausolus and his queen watch over the few broken remains of the beautiful tomb she built for him.\n",
      "\n",
      "Question: What is the location of the Colossus of Rhodes?\n",
      "Answer: The exact location of the Colossus of Rhodes is a matter of debate. While scholars generally agree that anecdotal depictions of the Colossus straddling the harbour's entry point have no historic or scientific basis, the monument's actual location remains a matter of debate. As mentioned above the statue is thought locally to have stood where two pillars now stand at the Mandraki port entrance. The floor of the Fortress of St Nicholas, near the harbour entrance, contains a circle of\n"
     ]
    }
   ],
   "source": [
    "# Usage: ask a question\n",
    "\n",
    "question = \"What does Rhodes Statue look like?\"\n",
    "\n",
    "response = basic_rag_pipeline.run({\"text_embedder\": {\"text\": question}, \"prompt_builder\": {\"question\": question}})\n",
    "    # diccionario con los datos de entrada para cada componente\n",
    "\n",
    "print(response[\"llm\"][\"replies\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo hemos podido realizar lo mismo que en el anterior:\n",
    "\n",
    "* Cargar un Document Retriever.\n",
    "* Añadir el embedder [pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb](https://huggingface.co/pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb) de HuggingFace en el formato Sentence Transformers.\n",
    "* Añadir una plantilla de prompt (context y query).\n",
    "* Añadir un LLM generativo de Huggingface.\n",
    "* Conectar el flujo de trabajo y lanzar una pregunta.\n",
    "\n",
    "#### Características\n",
    "\n",
    "* Tipos de Retrievers: keyword, dense embedding, sparse embedding (combinación de los 2 anteriores), filter, hybrid, multi-embedding.\n",
    "* Document Store: BD de documentos. Soporta BD local temporal/persistente y BD en remoto.\n",
    "* Converters. De cualquier formato a Document.\n",
    "* Evaluators: centrados en la evaluación de pipelines generativos. El más importante es RagasEvaluator.\n",
    "* Más información:\n",
    "  * Cookbook: https://github.com/deepset-ai/haystack-cookbook\n",
    "  * RAG usando PubMed: https://haystack.deepset.ai/blog/mixtral-8x7b-healthcare-chatbot\n",
    "  * RAGAS: Evaluación de modelos generativos https://arxiv.org/pdf/2309.15217"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferencias entre Haystack y Langchain\n",
    "\n",
    "* Haystack está centrado en retrieval (con integración de motores de búsqueda), Langchain en la integración de los LLMs en los flujos de trabajo (incluyendo agentes de decisión).\n",
    "* La documentación de Haystack es más completa. La de LangChain está organizada por conceptos. Por ejemplo no hay ningún retriever de keywords documentado en la API reference de Langchain (BM25 existe pero no aparece). El número de tutoriales y recetas en los cookbooks son similares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propuesta de evaluación de los métodos\n",
    "* [**MedSentEval**](https://pdf.sciencedirectassets.com/272371/1-s2.0-S1532046420X00037/1-s2.0-S1532046420300253/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHcaCXVzLWVhc3QtMSJGMEQCIDjAXDMFZQiUIFVyI9BPb6MV3Xxe3PZnBjDLUXMPq1FlAiBhXDXz442qRjW00Ymi8IgHvXU9XAb2BG6RbtMM3%2ByxXiq7BQjg%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMb7UsDJAHnCQ4ic0qKo8FhgKbrZCPefJYqJxbv3BKftfgqkMOJOUsMxErL%2F0%2BWGpG2srEZiSCx7YfTk4TcD%2BC%2BYBNqFPu%2BRlo%2Fd88BhhUUrMGyM2P0FjPmMQjqE6AZiRwNf%2F%2B5xLpvE9EaRF91VBmrH0m3RGlv8ww2gc3H%2FRh0yjkKP3soOhNwH8PkbS7lga2byw7nh8ZMtETwkPzpPWXPviuUENBrfVsvhwn1nCwqa%2B3m93t8ZFof60hYR73kS0uer%2FcOAcqjAaPNCznwOg6h0Mc3%2FYQ%2BPfaUPmhlttiTmz9Ln0E9ESZuqAWZ0VW5n%2B7iI7DHcXiDqKDqwXobAQDXxQj%2B5qOBPzQqMs1f90uOUKLRGKQYRZqgQV11nPa3sEf24zWNMpNiOxpS5lQwSoWkAqqspiBjNmRMH%2F3bBxTncavpr88m7BJpw55rQ4eFdYN8LkHv8JkfcJkRSmqiL7LBVqqeB2oJOELJv6odFOd%2BhNHpYppPTHzQ9y6ukHeQxhpSkfBRLfp022fk%2BnYHZuR7XhzdC9ohdHpEV0RfOYbZQBHNikmvQK6LBpVizfZaZjrkiJuOGg6ygrJptUyoRtADUWLS6caYxccYQBt%2BjMkngeEJq7p5NPUot8xMknuAmRgs0QiZvEKMKbwRMESVlp7vkfc2F%2B28bCXzcXeoxfhAEc45YVSLe4DX%2BeNSTwIvDZsh1It0yH29gNMXFPGhnPMqP43pmbfwoblFGMDAWYt6vFMUv63qxBuBvMC7QAZUDMxYnpL95SWpoVnCQsk5Pgj9mo1U9i%2FYhJJejf%2BFhRR6SNfc7d6PTvTI5LENiWul25u6ALZYhfxohIWnLyTV51tAbLVmHjq5ykyDr1JfFtqzeotkpmskUPIWo7ZCmdhBDCOmuu4BjqyAZxoWM8UbxZmZ2L3AAUcYjylkNRFNF%2FD4RlUE9ugOFSq6MBF57hPzPegXGDhLzd8neZOUkqtRXTzCAcb3cQxJHT74wSNaSNdOWKlURJ%2BVevx64VQCzYxT8%2BM1rbVnzTt3KDpMRVIZq2kVZ97jYneSqGGV4LRoybuzIZ7BxRMWvkEnaAlgvxTiJHMZzBpD5%2Fpj9UhPwrnBHvmZL5EktNrSMqw95l78d3dHs%2FBjFj30jn1gk8%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20241024T231550Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYU2YAB75G%2F20241024%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=404eb5439c56f36f775d07bfc32a3c4392b8f180370f19be80ecc5109d24a26f&hash=3edc86f2bc46ac7d10d13ed44bec5808eadd5c4f08ee76914abd207a0fceb5b9&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1532046420300253&tid=spdf-cd65e576-7648-463a-a062-55b2d4cf9dd3&sid=9864b0b818409847ca885db8f1043cc82732gxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=1f155605550b0e02030157&rr=8d7dabce1e1a314b&cc=es) fue un experimento de evaluación unificada de sentence embeddings en el dominio médico. Se evaluaron los modelos de representación textual: GloVe, FastText, ELMO, BERT, InferSent, USE así como algunas de sus versiones especializadas en un dominio médico o científico como BioBERT, SciBERT, InferSent_{MedNLI} o ELMO_{PubMed}. Para evaluarlos se probaron las siguientes tareas con datasets de test especializados: Text Entailment, RQE, Sentence classification, Sentiment analysis, QA y STS. Por ejemplo:\n",
    "* Clinical STS (C-STS) y BIOSSES datasets etiquetados para STS. Se evaluaron los modelos con la métrica de correlación de Pearson/Spearman entre la similitud coseno y la gold label.\n",
    "* MedNLI: TE.\n",
    "* PICO: corpus etiquetado de oraciones de abstracts de PubMed para clasificación. \n",
    "\n",
    "**Problema**: El modelo pritamdeka (https://huggingface.co/pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb) fue fine-tuneado en algunos de estos datasets.\n",
    "\n",
    "Un tutorial del código se puede ver en https://github.com/nstawfik/MedSentEval/blob/master/MedSentEval_Tutorial.ipynb. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
