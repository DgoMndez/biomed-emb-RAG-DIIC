{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG: Retrieval-Augmented Generation\n",
    "\n",
    "## ¬øQu√© es RAG?\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) es una extensi√≥n de un LLM que combina la recuperaci√≥n (Retrieval) con la generaci√≥n (Generation) para mejorar la adecuaci√≥n de las respuestas. Se trata de incorporar un m√≥dulo de recuperaci√≥n al LLM generativo que se encarga de dada una query buscar documentos relevantes sobre ella en una fuente determinada (BD, corpus, online) y utlizarlos para crear una query m√°s precisa que es la que se transmite al modelo generativo para generar la respuesta.\n",
    "\n",
    "Sus posibles impactos son:\n",
    "* Coste bajo respecto al entrenamiento.\n",
    "* Mejora de la precisi√≥n en las respuestas, al usar fuentes externas relacionadas con la query.\n",
    "* F√°cil actualizaci√≥n: se puede actualizar solo la BD de documentos en lugar de reentrenar el modelo.\n",
    "\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "Nuestro objetivo es la especializaci√≥n de LLMs en el dominio biom√©dico.\n",
    "\n",
    "El a√±o pasado estuvimos trabajando en la especializaci√≥n de los embeddings de BERT ([pritamdeka](https://huggingface.co/pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb)) en el dominio de [HPO](https://hpo.jax.org/). Para lo que construimos un corpus de abstracts de [PubMed](https://pubmed.ncbi.nlm.nih.gov/) etiquetados con su fenotipo de b√∫squeda, entrenamos el modelo mediante fine-tuning y evaluamos el aprendizaje (de la codificaci√≥n de fenotipos) a lo largo del proceso. Los principales problemas / aspectos de mejora fueron:\n",
    "\n",
    "* El m√©todo de evaluaci√≥n estaba centrado √∫nicamente en la codificaci√≥n de los fenotipos en embeddings. Se basaba en la comparaci√≥n de la similitud coseno entre los embeddings de los t√©rminos (similitud inferida) con la similitud Lin de los fenotipos en HPO (similitud de referencia). Se requiere un m√©todo de evaluaci√≥n m√°s estandarizado que mida si el modelo comprende los textos biom√©dicos. Por ejemplo [MedSentEval](https://www.sciencedirect.com/science/article/pii/S1532046420300253?via%3Dihub).\n",
    "* El error de test en realidad era de validaci√≥n, porque se us√≥ el conjunto de test para encontrar los mejores hiperpar√°metros.\n",
    "* No se monitoriz√≥ la p√©rdida a lo largo del proceso. Aunque guarda cierta relaci√≥n con el m√©todo de evaluaci√≥n es diferente y puede aportar m√°s informaci√≥n sobre si hay underfitting/overfitting.\n",
    "* El vocabulario del modelo no se expande mediante el fine-tuning, y muchos t√©rminos de HPO no est√°n en el vocabulario de BERT.\n",
    "\n",
    "El nuevo enfoque para tratar la especializaci√≥n de LLMs en medicina ser√° el RAG. El m√≥dulo de recuperaci√≥n debe buscar informaci√≥n relevante sobre la query en un corpus de documentos de PubMed. Si no se usa la Generaci√≥n, se trata de un modelo de Recuperaci√≥n Aumentada (Retrieval-Augmented). Este tipo de modelos son utilizados para clasificaci√≥n, extracci√≥n de informaci√≥n y QA.\n",
    "\n",
    "## Librer√≠as a utilizar\n",
    "\n",
    "Se propone usar alguna de las siguientes librer√≠as de python para RAG:\n",
    "\n",
    "1. Langchain.\n",
    "2. Haystack.\n",
    "\n",
    "Las dos librer√≠as permiten construir sistemas RAG integrando distintos modelos LLM, generativos o embedder de OpenAI, Cohere, Google, HuggingFace...\n",
    "\n",
    "### 1. Langchain\n",
    "\n",
    "\"LangChain es una biblioteca dise√±ada para ayudar a construir aplicaciones que integran modelos de lenguaje grandes (LLMs), como GPT, con fuentes de datos externas y flujos de trabajo personalizados. Ofrece una infraestructura flexible para crear aplicaciones que utilizan LLMs de manera eficiente, permitiendo conectar estos modelos con bases de datos, APIs y otros recursos externos, as√≠ como organizar la interacci√≥n en diferentes etapas o flujos.\" - ChatGPT\n",
    "\n",
    "[Langchain](https://www.langchain.com/) es una empresa de San Francisco dedicada a dar el soporte necesario a los negocios para construir aplicaciones sobre LLMs. Sus tres productos principales son:\n",
    "\n",
    "* [Langchain](https://www.langchain.com/langchain): es la librer√≠a para el desarrollo de las aplicaciones basadas en LLMs. Permite construir la estructura de la aplicaci√≥n conectando los m√≥dulos de modelos, BBDD, fuentes externas, prompts... \n",
    "* [Langsmith](https://www.langchain.com/langsmith): asistencia al desarrollo de software basado en LLMs a lo largo de todo su ciclo de vida. Sirve para debug, colaboraci√≥n, test y monitorizaci√≥n. Puede ser √∫til para la explicaci√≥n y la evaluaci√≥n de nuestro modelo.\n",
    "* [Langgraph](https://www.langchain.com/langgraph): API para el desarrollo de agentes aut√≥nomos. Sirven para la toma de decisiones o la asistencia. En principio no lo vamos a usar para nada.\n",
    "\n",
    "#### T√©rminos y condiciones de uso.\n",
    "\n",
    "Seg√∫n las FAQ, LangChain es open-source.\n",
    "\n",
    "Los otros servicios de esta empresa (LangGraph Cloud, LangSmith) est√°n sujetos a una licencia que es poco restrictiva. El √∫nico punto delicado es:\n",
    "\n",
    "\"Customer may not directly or indirectly and may not authorize any third party to: (a) decompile, disassemble, reverse engineer, or otherwise attempt to derive the source code, structure, ideas, algorithms, or associated know-how of, the Licensed Platform, Documentation, or reconstruct, or discover, any hidden or non-public elements of the Licensed Platform (except to the extent expressly permitted by applicable law not withstanding this restriction);\"\n",
    "\n",
    "\"(d) use the Licensed Platform to develop a similar or competing product or service\"\n",
    "\n",
    "Pero no nos obstaculiza en nada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get started\n",
    "* Cargamos las variables de entorno: API keys y logging info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar .env\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "\n",
    "WS_ROUTE = \"../\"\n",
    "load_dotenv(WS_ROUTE + \"env/diic.env\")\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")\n",
    "\n",
    "if \"HF_API_KEY\" not in os.environ:\n",
    "    os.environ[\"HF_API_KEY\"] = getpass(\"Enter Hugging Face API key:\")\n",
    "    \n",
    "if \"COHERE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"COHERE_API_KEY\"] = getpass(\"Enter Cohere API key:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# General\\n%pip install --quiet --upgrade langchain langchain-community langchain-chroma\\n    \\n# OpenAI\\n%pip install -qU langchain-openai\\n    \\n# Cohere\\n%pip install -qU langchain-cohere\\n\\n# HuggingFace\\n%pip install -qU langchain-huggingface\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instalaci√≥n\n",
    "\"\"\"\n",
    "# General\n",
    "%pip install --quiet --upgrade langchain langchain-community langchain-chroma\n",
    "    \n",
    "# OpenAI\n",
    "%pip install -qU langchain-openai\n",
    "    \n",
    "# Cohere\n",
    "%pip install -qU langchain-cohere\n",
    "\n",
    "# HuggingFace\n",
    "%pip install -qU langchain-huggingface\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Modelos de OpenAI.\n",
    "No tengo acceso a la API de OpenAI: salta el error \"Rate Limit Exceeded, insufficient quota\" para distintos modelos.\n",
    "Otra desventaja: seg√∫n sus T√©rminos y Condiciones de uso est√° prohibido usar el output de sus modelos para desarrollar otros modelos que compitan con OpenAI (\"Using Output to develop models that compete with OpenAI.\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo\n",
      "Rate limit exceeded: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "o1\n",
      "Rate limit exceeded: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    }
   ],
   "source": [
    "# Modelos de OpenAI\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llms = [ChatOpenAI(model_name=\"gpt-3.5-turbo\"), ChatOpenAI(model_name=\"o1\")]\n",
    "\n",
    "for llm in llms:\n",
    "    print(llm.model_name)\n",
    "    try:\n",
    "        llm = ChatOpenAI()\n",
    "        llm.invoke(\"Hello, world!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Rate limit exceeded: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n",
      "\n",
      "I really need to talk about this, but we already discussed so far.\n",
      "\n",
      "So many things about you that I don't really feel like writing about here. You already do your first episode with us but there are\n"
     ]
    }
   ],
   "source": [
    "# Alternativas: modelos accesibles\n",
    "\n",
    "# 1. Sentence transformers\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Cargar el modelo de Hugging Face\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Generar texto\n",
    "response = generator(\"Hello, world!\", max_length=50)\n",
    "print(response[0]['generated_text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u: Hello, world!\n",
      "Hello, how can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# 2. Cohere\n",
    "import cohere\n",
    "\n",
    "co = cohere.Client(os.environ[\"COHERE_API_KEY\"])\n",
    "\n",
    "# Generar texto\n",
    "response = co.generate(\n",
    "    model='command-r-plus-08-2024',\n",
    "    prompt='Hello, world!',\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "print(\"u: Hello, world!\")\n",
    "print(response.generations[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tutorial RAG Langchain\n",
    "Fuente: https://python.langchain.com/docs/tutorials/rag/\n",
    "https://github.com/langchain-ai/langchain/blob/master/docs/docs/tutorials/rag.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos Cohere para generar texto\n",
    "\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "llm = ChatCohere(model=\"command-r-plus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u: What is Task Decomposition?\n",
      "C: Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\n",
      "\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "\n",
      "Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n",
      "\n",
      "\n",
      "Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\n",
      "\n",
      "Fig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
      "The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\n",
      "Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent‚Äôs working memory, up to three, to be used as context for querying LLM.\n",
      "-----\n",
      "Prompt: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "\n",
      "            Question: What is Task Decomposition?\n",
      "\n",
      "            Context: Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\n",
      "\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "\n",
      "Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n",
      "\n",
      "\n",
      "Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\n",
      "\n",
      "Fig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
      "The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\n",
      "Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent‚Äôs working memory, up to three, to be used as context for querying LLM.\n",
      "Answer:\n",
      "-----\n",
      "R: Task decomposition is a technique used to break down complex tasks into smaller, more manageable steps, often employed to enhance model performance on challenging assignments. This method can be executed by LLMs through simple prompting, task-specific instructions, or human input.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings # No accesible\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "# 1. Indexing Load\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2. Indexing split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3. Indexing store\n",
    "# No tengo acceso a OpenAIEmbeddings\n",
    "#vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Usar Hugging Face Embeddings en lugar de OpenAIEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)\n",
    "\n",
    "# 4. Retrieve and 5. Generate\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "ptemplate = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\n",
    "            Question: {question}\\n\n",
    "            Context: {context}\\nAnswer:\"\"\"\n",
    "            \n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=ptemplate)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "query = \"What is Task Decomposition?\"\n",
    "print(f\"u: {query}\")\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "context = format_docs(retrieved_docs)\n",
    "print(\"C: \" + context)\n",
    "print(\"-----\")\n",
    "print(\"Prompt: \" + ptemplate.format(context=context, question=query))\n",
    "print(\"-----\")\n",
    "print(\"R: \" + response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caracter√≠sticas de LangChain\n",
    "\n",
    "1. Fuentes externas: en el ejemplo anterior se accede a una web para conseguir los documentos que conforman la BD del retriever.\n",
    "2. Cadenas/chains: en el ejemplo anterior se crea una cadena de RAG formada por: retriever, formateador, prompt, llm y parser. Langchain permite crear flujos de trabajo tan complejos como se quiera usando los diferentes tipos de m√≥dulos. Hay un gran cat√°logo de retrievers, modelos y prompts a utilizar. Nos interesan los modelos de HuggingFace porque son de libre acceso. En el modelo por ejemplo se us√≥ un embedder de HuggingFace, un Vector Retriever y un LLM generativo de Cohere. El LLM de OpenAI requiere una clave de acceso con suficiente cuota en el plan.\n",
    "3. Agentes: esta es la parte correspondiente a LangGraph que en principio no se va a usar. Sirve para que las cadenas tomen decisiones para dirigir el flujo de trabajo, por ejemplo cu√°nta informaci√≥n buscar, d√≥nde buscarla, cu√°ndo devolver una respuesta con la salida generada o cu√°ndo iterar para mejorarla...\n",
    "4. Memoria: para agentes conversacionales, LangChain ofrece soporte para recordar las interacciones y adaptar as√≠ sus respuestas.\n",
    "\n",
    "M√°s informaci√≥n:\n",
    "* Deconstructing RAG: https://blog.langchain.dev/deconstructing-rag/.\n",
    "* RAG strategies: https://blog.langchain.dev/applying-openai-rag/.\n",
    "* RAG evaluation cookbook: https://github.com/langchain-ai/langchain/blob/master/cookbook/advanced_rag_eval.ipynb?ref=blog.langchain.dev\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Haystack\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haystack es una librer√≠a de c√≥digo abierto de Deepset dedicada al mismo problema que Langchain: soportar el desarrollo de apps basadas en LLMs. Permite pr√°cticamente la misma funcionalidad: crear flujos de trabajo combinando m√≥dulos como LLMs, embedders, retrievers y BD de documentos, donde para cada uno se tienen distintas opciones como HuggingFace, OpenAI o Cohere. Tambi√©n ofrece data ingestion (obtener los datos online), evaluaci√≥n y monitorizaci√≥n.\n",
    "\n",
    "\n",
    "#### Tutorial RAG con Haystack\n",
    "[27_First_RAG_Pipeline.ipynb](./haystack/27_First_RAG_Pipeline.ipynb) basado en https://haystack.deepset.ai/tutorials/27_first_rag_pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a237f5f7e04e0c881a982328b5cf99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Database\n",
    "\n",
    "# Document Store\n",
    "\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from haystack import Document\n",
    "\n",
    "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
    "docs = [Document(content=doc[\"content\"], meta=doc[\"meta\"]) for doc in dataset]\n",
    "\n",
    "# Document embedder\n",
    "\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "\n",
    "doc_embedder = SentenceTransformersDocumentEmbedder(model=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
    "doc_embedder.warm_up()\n",
    "\n",
    "# Write documents\n",
    "\n",
    "docs_with_embeddings = doc_embedder.run(docs)\n",
    "document_store.write_documents(docs_with_embeddings[\"documents\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG components\n",
    "\n",
    "# 1. Embedder (same as Document Store)\n",
    "\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "\n",
    "text_embedder = SentenceTransformersTextEmbedder(model=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
    "\n",
    "# 2. Retriever\n",
    "\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "\n",
    "retriever = InMemoryEmbeddingRetriever(document_store)\n",
    "\n",
    "# 3. Prompt\n",
    "\n",
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "template = \"\"\"\n",
    "Given the following information, answer the question.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_builder = PromptBuilder(template=template)\n",
    "\n",
    "\n",
    "# 4. LLM Generator\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# No tengo acceso a OpenAI\n",
    "#from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.generators import HuggingFaceAPIGenerator\n",
    "from haystack.utils import Secret\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")\n",
    "\n",
    "if \"HF_API_KEY\" not in os.environ:\n",
    "    os.environ[\"HF_API_KEY\"] = getpass(\"Enter Hugging Face API key:\")\n",
    "\n",
    "#generator = OpenAIGenerator(model=\"text-embedding-ada-002\")\n",
    "generator = HuggingFaceAPIGenerator(api_type=\"serverless_inference_api\",\n",
    "                                    api_params={\"model\": \"HuggingFaceH4/zephyr-7b-beta\"},\n",
    "                                    token=Secret.from_token(os.environ[\"HF_API_KEY\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x729c61541050>\n",
       "üöÖ Components\n",
       "  - text_embedder: SentenceTransformersTextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: HuggingFaceAPIGenerator\n",
       "üõ§Ô∏è Connections\n",
       "  - text_embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG Pipeline\n",
    "\n",
    "from haystack import Pipeline\n",
    "\n",
    "basic_rag_pipeline = Pipeline()\n",
    "# Add components to your pipeline\n",
    "basic_rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "basic_rag_pipeline.add_component(\"retriever\", retriever)\n",
    "basic_rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "basic_rag_pipeline.add_component(\"llm\", generator)\n",
    "\n",
    "# Now, connect the components to each other\n",
    "basic_rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "basic_rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\") #.documents -> puntos de entrada\n",
    "basic_rag_pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868695995b7b4f78b5ef094ecfe16fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Rhodes Statue, also known as the Colossus of Rhodes, was a statue of the Greek sun-god Helios, erected in the city of Rhodes by Chares of Lindos in 280 BC. It was constructed to celebrate the successful defence of Rhodes city against an attack by Demetrius Poliorcetes, who had besieged it for a year with a large army and navy. According to most contemporary descriptions, the Colossus stood approximately 70 cubits, or 33 metres (108 feet) high ‚Äì approximately the height of the modern Statue of Liberty from feet to crown ‚Äì making it the tallest statue in the ancient world. It collapsed during the earthquake of 226 BC, although parts of it were preserved. In accordance with a certain oracle, the Rhodians did not build it again. John Malalas wrote that Hadrian in his reign re-erected the Colossus, but he was mistaken. According to the Suda, the Rhodians were called Colossaeans (ŒöŒøŒªŒøœÉœÉŒ±Œµ·øñœÇ), because they erected the statue on the island. Today, the massive castle of the Knights Hospitaller (Knights of St. John) still stands in Bodrum, and the polished stone and marble blocks of the Mausoleum can be spotted built into the walls of the structure. At the site of the Mausoleum, only the foundation remains, and a small museum. Some of the surviving sculptures at the British Museum include fragments of statues and many slabs of the frieze showing the battle between the Greeks and the Amazons. There the images of Mausolus and his queen watch over the few broken remains of the beautiful tomb she built for him.\n",
      "\n",
      "Question: What is the location of the Colossus of Rhodes?\n",
      "Answer: The exact location of the Colossus of Rhodes is a matter of debate. While scholars generally agree that anecdotal depictions of the Colossus straddling the harbour's entry point have no historic or scientific basis, the monument's actual location remains a matter of debate. As mentioned above the statue is thought locally to have stood where two pillars now stand at the Mandraki port entrance. The floor of the Fortress of St Nicholas, near the harbour entrance, contains a circle of\n"
     ]
    }
   ],
   "source": [
    "# Usage: ask a question\n",
    "\n",
    "question = \"What does Rhodes Statue look like?\"\n",
    "\n",
    "response = basic_rag_pipeline.run({\"text_embedder\": {\"text\": question}, \"prompt_builder\": {\"question\": question}})\n",
    "    # diccionario con los datos de entrada para cada componente\n",
    "\n",
    "print(response[\"llm\"][\"replies\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo hemos podido realizar lo mismo que en el anterior:\n",
    "\n",
    "* Cargar un Document Retriever.\n",
    "* A√±adir el embedder [pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb](https://huggingface.co/pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb) de HuggingFace en el formato Sentence Transformers.\n",
    "* A√±adir una plantilla de prompt (context y query).\n",
    "* A√±adir un LLM generativo de Huggingface.\n",
    "* Conectar el flujo de trabajo y lanzar una pregunta.\n",
    "\n",
    "#### Caracter√≠sticas\n",
    "\n",
    "* Tipos de Retrievers: keyword, dense embedding, sparse embedding (combinaci√≥n de los 2 anteriores), filter, hybrid, multi-embedding.\n",
    "* Document Store: BD de documentos. Soporta BD local temporal/persistente y BD en remoto.\n",
    "* Converters. De cualquier formato a Document.\n",
    "* Evaluators: centrados en la evaluaci√≥n de pipelines generativos. El m√°s importante es RagasEvaluator.\n",
    "* M√°s informaci√≥n:\n",
    "  * Cookbook: https://github.com/deepset-ai/haystack-cookbook\n",
    "  * RAG usando PubMed: https://haystack.deepset.ai/blog/mixtral-8x7b-healthcare-chatbot\n",
    "  * RAGAS: Evaluaci√≥n de modelos generativos https://arxiv.org/pdf/2309.15217"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferencias entre Haystack y Langchain\n",
    "\n",
    "* Haystack est√° centrado en retrieval (con integraci√≥n de motores de b√∫squeda), Langchain en la integraci√≥n de los LLMs en los flujos de trabajo (incluyendo agentes de decisi√≥n).\n",
    "* La documentaci√≥n de Haystack es m√°s completa. La de LangChain est√° organizada por conceptos. Por ejemplo no hay ning√∫n retriever de keywords documentado en la API reference de Langchain (BM25 existe pero no aparece). El n√∫mero de tutoriales y recetas en los cookbooks son similares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propuesta de evaluaci√≥n de los m√©todos\n",
    "* [**MedSentEval**](https://pdf.sciencedirectassets.com/272371/1-s2.0-S1532046420X00037/1-s2.0-S1532046420300253/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHcaCXVzLWVhc3QtMSJGMEQCIDjAXDMFZQiUIFVyI9BPb6MV3Xxe3PZnBjDLUXMPq1FlAiBhXDXz442qRjW00Ymi8IgHvXU9XAb2BG6RbtMM3%2ByxXiq7BQjg%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMb7UsDJAHnCQ4ic0qKo8FhgKbrZCPefJYqJxbv3BKftfgqkMOJOUsMxErL%2F0%2BWGpG2srEZiSCx7YfTk4TcD%2BC%2BYBNqFPu%2BRlo%2Fd88BhhUUrMGyM2P0FjPmMQjqE6AZiRwNf%2F%2B5xLpvE9EaRF91VBmrH0m3RGlv8ww2gc3H%2FRh0yjkKP3soOhNwH8PkbS7lga2byw7nh8ZMtETwkPzpPWXPviuUENBrfVsvhwn1nCwqa%2B3m93t8ZFof60hYR73kS0uer%2FcOAcqjAaPNCznwOg6h0Mc3%2FYQ%2BPfaUPmhlttiTmz9Ln0E9ESZuqAWZ0VW5n%2B7iI7DHcXiDqKDqwXobAQDXxQj%2B5qOBPzQqMs1f90uOUKLRGKQYRZqgQV11nPa3sEf24zWNMpNiOxpS5lQwSoWkAqqspiBjNmRMH%2F3bBxTncavpr88m7BJpw55rQ4eFdYN8LkHv8JkfcJkRSmqiL7LBVqqeB2oJOELJv6odFOd%2BhNHpYppPTHzQ9y6ukHeQxhpSkfBRLfp022fk%2BnYHZuR7XhzdC9ohdHpEV0RfOYbZQBHNikmvQK6LBpVizfZaZjrkiJuOGg6ygrJptUyoRtADUWLS6caYxccYQBt%2BjMkngeEJq7p5NPUot8xMknuAmRgs0QiZvEKMKbwRMESVlp7vkfc2F%2B28bCXzcXeoxfhAEc45YVSLe4DX%2BeNSTwIvDZsh1It0yH29gNMXFPGhnPMqP43pmbfwoblFGMDAWYt6vFMUv63qxBuBvMC7QAZUDMxYnpL95SWpoVnCQsk5Pgj9mo1U9i%2FYhJJejf%2BFhRR6SNfc7d6PTvTI5LENiWul25u6ALZYhfxohIWnLyTV51tAbLVmHjq5ykyDr1JfFtqzeotkpmskUPIWo7ZCmdhBDCOmuu4BjqyAZxoWM8UbxZmZ2L3AAUcYjylkNRFNF%2FD4RlUE9ugOFSq6MBF57hPzPegXGDhLzd8neZOUkqtRXTzCAcb3cQxJHT74wSNaSNdOWKlURJ%2BVevx64VQCzYxT8%2BM1rbVnzTt3KDpMRVIZq2kVZ97jYneSqGGV4LRoybuzIZ7BxRMWvkEnaAlgvxTiJHMZzBpD5%2Fpj9UhPwrnBHvmZL5EktNrSMqw95l78d3dHs%2FBjFj30jn1gk8%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20241024T231550Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYU2YAB75G%2F20241024%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=404eb5439c56f36f775d07bfc32a3c4392b8f180370f19be80ecc5109d24a26f&hash=3edc86f2bc46ac7d10d13ed44bec5808eadd5c4f08ee76914abd207a0fceb5b9&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1532046420300253&tid=spdf-cd65e576-7648-463a-a062-55b2d4cf9dd3&sid=9864b0b818409847ca885db8f1043cc82732gxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=1f155605550b0e02030157&rr=8d7dabce1e1a314b&cc=es) fue un experimento de evaluaci√≥n unificada de sentence embeddings en el dominio m√©dico. Se evaluaron los modelos de representaci√≥n textual: GloVe, FastText, ELMO, BERT, InferSent, USE as√≠ como algunas de sus versiones especializadas en un dominio m√©dico o cient√≠fico como BioBERT, SciBERT, InferSent_{MedNLI} o ELMO_{PubMed}. Para evaluarlos se probaron las siguientes tareas con datasets de test especializados: Text Entailment, RQE, Sentence classification, Sentiment analysis, QA y STS. Por ejemplo:\n",
    "* Clinical STS (C-STS) y BIOSSES datasets etiquetados para STS. Se evaluaron los modelos con la m√©trica de correlaci√≥n de Pearson/Spearman entre la similitud coseno y la gold label.\n",
    "* MedNLI: TE.\n",
    "* PICO: corpus etiquetado de oraciones de abstracts de PubMed para clasificaci√≥n. \n",
    "\n",
    "**Problema**: El modelo pritamdeka (https://huggingface.co/pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb) fue fine-tuneado en algunos de estos datasets.\n",
    "\n",
    "Un tutorial del c√≥digo se puede ver en https://github.com/nstawfik/MedSentEval/blob/master/MedSentEval_Tutorial.ipynb. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
